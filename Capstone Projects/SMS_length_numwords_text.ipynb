{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Python script for confusion matrix creation \n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_dict(d, **kwargs):\n",
    "    updated_d = d.copy()\n",
    "    updated_d.update(kwargs)\n",
    "    return updated_d\n",
    "\n",
    "def clean_func_names(df):\n",
    "    df = df.copy()   \n",
    "    columns = [c for c in df.columns if callable(df[c][0])] \n",
    "    for c in columns:\n",
    "        funcs = []\n",
    "        for train in df[c]:\n",
    "            funcs.append(train.__name__)\n",
    "        df[c] = funcs\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparamaters:\n",
    "SCALED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('SMSSpamCollection.txt', sep='\\t', header=None, names=['spam', 'text'])\n",
    "\n",
    "# set categorical values of spam to 0 or 1\n",
    "df['spam'] = df['spam'] == 'spam' # makes True/False instead of \"spam\" and \"ham\"\n",
    "df['spam'] = df['spam'].astype(int)  # number values instead of boolean value\n",
    "\n",
    "# Adding new feature 'length'\n",
    "L = []\n",
    "for i in df.text:\n",
    "    L.append(len(i))\n",
    "df['length'] = L\n",
    "\n",
    "# Add second engineered feature 'num_words'\n",
    "words = df.copy()\n",
    "num_words = []\n",
    "for i in range(len(words.text)):\n",
    "    value = words['text'][i].split(' ')\n",
    "    num_words.append(len(value))\n",
    "num_words\n",
    "df['num_words'] = num_words\n",
    "\n",
    "#Create sub DataFrame\n",
    "sub_df = df[['text', 'length', 'num_words']]\n",
    "\n",
    "# Split train test\n",
    "X_train,  X_test, y_train, y_test =  train_test_split(sub_df, df.spam.values, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to scale features, as we have just one. However, in later models we use more than just this features and therefore as exercise, we do it right now as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\milen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Use TweetTokenizer \n",
    "tknzr = TweetTokenizer()\n",
    "X_train['text'] = X_train.text.apply(tknzr.tokenize)\n",
    "X_test['text'] = X_test.text.apply(tknzr.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = X_train.text\n",
    "text_test = X_test.text\n",
    "\n",
    "# We splitted data and therefore we need to get list of indices to iterate over if we want to create 'documents' variable\n",
    "id_tr = list(text_train.index.values)\n",
    "id_tr.sort()\n",
    "doc_train = []\n",
    "for i in id_tr:\n",
    "    doc_train.append(' '.join(text_train[i]))\n",
    "    \n",
    "# The same for test set\n",
    "id_ts = list(text_test.index.values)\n",
    "id_ts.sort()\n",
    "\n",
    "doc_test = []\n",
    "for i in id_ts:\n",
    "    doc_test.append(' '.join(text_test[i]))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method .toarray() assures that we gain dense matrix\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_text_tr = tfidf.fit_transform(doc_train).toarray()\n",
    "X_text_ts = tfidf.transform(doc_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_text_tr)\n",
    "X_text_tr_pca = pca.transform(X_text_tr)\n",
    "X_text_ts_pca = pca.transform(X_text_ts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['length', 'num_words']].values\n",
    "X_test = X_test[['length', 'num_words']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# We will used MinMaxScaler, which scales values in a way that our new values will be within itnerval <0,1>.\n",
    "# ATTENTION! With train set we use .fit_transform method(), with test set only .transform()!!!\n",
    "if SCALED == True:\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_sc = scaler.fit_transform(X_train)#.reshape(-1, 1))\n",
    "    X_test_sc = scaler.transform(X_test)#.reshape(-1, 1))\n",
    "#else:\n",
    " #   X_train_sc = X_train.reshape(-1, 1)\n",
    "     #X_test_sc = X_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = np.concatenate((X_train_sc, X_text_tr), axis=1)\n",
    "X_test_sc = np.concatenate((X_test_sc, X_text_ts), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(X_train_sc, y_train, **kwargs):\n",
    "    clf = LogisticRegression(random_state=0, \n",
    "                             class_weight='balanced',\n",
    "                             solver=kwargs.get('solver', 'sag'), # instead of defaults you can have {} which would rise exception\n",
    "                             penalty=kwargs.get('penalty', 'l2'), \n",
    "                             C=kwargs.get('C', 1.0))\n",
    "    model = clf.fit(X_train_sc, y_train)\n",
    "    return model \n",
    "\n",
    "def train_SVM(X_train_sc, y_train, **kwargs):\n",
    "    clf = SVC(random_state=0, \n",
    "                             class_weight='balanced',\n",
    "                             kernel=kwargs.get('kernel', 'rbf'), # instead of defaults you can have {} which would rise exception\n",
    "                             coef0=kwargs.get('coef0', 0.0), \n",
    "                             C=kwargs.get('C', 1.0))\n",
    "    model = clf.fit(X_train_sc, y_train)\n",
    "    return model \n",
    "\n",
    "def train_random(X_train_sc, y_train, **kwargs):\n",
    "    clf = RandomForestClassifier(random_state=0, class_weight='balanced', \n",
    "                                 bootstrap=kwargs.get('bootstrap', 'True'),\n",
    "                                 n_estimators=kwargs.get('n_estimators', 100))\n",
    "   \n",
    "    model = clf.fit(X_train_sc, y_train)\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return as pandas series, with multiple evaulation metrcis (fp, tn, fn, tp)\n",
    "def eval_model(X_test_sc, y_test, X_train_sc, y_train):\n",
    "    hp['test_score'] =  model.score(X_test_sc, y_test)\n",
    "    hp['train_score'] = model.score(X_train_sc, y_train) \n",
    "    hp['tn'], hp['fp'], hp['fn'], hp['tp'] = confusion_matrix(y_test, model.predict(X_test_sc)).ravel()\n",
    "    hp['auc score'] =   roc_auc_score(y_test, model.predict(X_test_sc))\n",
    "    hp['f1_score'] = f1_score(y_test, model.predict(X_test_sc), average='weighted', labels=np.unique(model.predict(X_test_sc)))\n",
    "    hp['recall'] = recall_score(y_test, model.predict(X_test_sc), average='weighted', labels=np.unique(model.predict(X_test_sc)))\n",
    "    hp['precision'] = precision_score(y_test, model.predict(X_test_sc), average='weighted', labels=np.unique(model.predict(X_test_sc)))\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hp['test_score'] =  eval_model(X_test_sc, y_test)\n",
    "    hp['train_score'] = eval_model(X_train_sc, y_train) \n",
    "    hp['tn'], hp['fp'], hp['fn'], hp['tp'] = confusion_matrix(y_test, model.predict(X_test_sc)).ravel()\n",
    "    hp['auc score'] =   roc_auc_score(y_test, model.predict(X_test_sc))\n",
    "    scores.append(hp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_function': <function train_SVM at 0x000001E05790EEA0>, 'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0, 'kernel': 'rbf', 'coef0': 0.0}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "logreg_def_hyperpar = dict(train_function=train_logreg, solver='liblinear', penalty='l1', C=1.0)\n",
    "SVM_def_hyperpar = dict(train_function=train_SVM, kernel='rbf', C=1.0, coef0=0.0)\n",
    "RF_def_hyperpar = dict(train_function=train_random, bootstrap=True, n_estimators=1)\n",
    "\n",
    "SVM_hyperparameters = [SVM_def_hyperpar]#, updated_dict(SVM_def_hyperpar, coef0=0.5),\n",
    "                     #  updated_dict(SVM_def_hyperpar, C=0.5),\n",
    "                    #   updated_dict(SVM_def_hyperpar,C=0.5, coef0=0.5),\n",
    "                   #    updated_dict(SVM_def_hyperpar, C=0.1),\n",
    "                   #    updated_dict(SVM_def_hyperpar, coef0=0.5),\n",
    "                      # updated_dict(SVM_def_hyperpar, kernel='linear'),\n",
    "                       #updated_dict(SVM_def_hyperpar,kernel='sigmoid')\n",
    "                   #   ]   \n",
    "\n",
    "RF_hyperparameters =[RF_def_hyperpar,updated_dict(RF_def_hyperpar, n_estimators=10),\n",
    "                     updated_dict(RF_def_hyperpar, n_estimators=20),\n",
    "                     updated_dict(RF_def_hyperpar, n_estimators=50),\n",
    "                     updated_dict(RF_def_hyperpar, n_estimators=100)]\n",
    "                   # ]#, updated_dict(RF_def_hyperpar,bootstrap=False),\n",
    "                    #updated_dict(RF_def_hyperpar,n_estimators=1),\n",
    "                    # updated_dict(RF_def_hyperpar,n_estimators=1000),\n",
    "                   #  updated_dict(RF_def_hyperpar,n_estimators=50),\n",
    "                  #   updated_dict(RF_def_hyperpar,n_estimators=1000, bootstrap=False)\n",
    "                  #  ]\n",
    "logreg_hyperparameters = [logreg_def_hyperpar]#, updated_dict(logreg_def_hyperpar, C=0.5),\n",
    "            #             updated_dict(logreg_def_hyperpar, C=0.1),\n",
    "                   #      updated_dict(logreg_def_hyperpar,solver='sag', penalty='l2'),\n",
    "          #               updated_dict(logreg_def_hyperpar, solver='sag', penalty='l2', C=0.5),\n",
    "           #              updated_dict(logreg_def_hyperpar,solver='sag', penalty='l2', C=0.1),\n",
    "                  #       updated_dict(logreg_def_hyperpar,solver='newton-cg', penalty='l2'),\n",
    "         #                updated_dict(logreg_def_hyperpar,solver='newton-cg', penalty='l2', C=0.5),\n",
    "        #                 updated_dict(logreg_def_hyperpar, solver='newton-cg', penalty='l2', C=0.1),\n",
    "       #                  updated_dict(logreg_def_hyperpar,solver='lbfgs', penalty='l2'),  \n",
    "      #                   updated_dict(logreg_def_hyperpar,solver='lbfgs', penalty='l2', C=0.5),\n",
    "     #                     updated_dict(logreg_def_hyperpar,solver='lbfgs', penalty='l2', C=0.1)\n",
    "                     #    ]\n",
    "\n",
    "\n",
    "for hyperparameters in SVM_hyperparameters:#(logreg_hyperparameters + SVM_hyperparameters + RF_hyperparameters):\n",
    "    hp = logreg_def_hyperpar.copy()\n",
    "    hp.update(hyperparameters)\n",
    "    train = hp.get('train_function')\n",
    "    print(hp)\n",
    "    model = train(X_train_sc, y_train, **hp)\n",
    "    hp = eval_model(X_test_sc, y_test, X_train_sc, y_train)\n",
    "    scores.append(hp)\n",
    "print(\"I am done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_func_names(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = clean_func_names(pd.DataFrame(scores))\n",
    "\n",
    "#df = df[['train_function','precision', 'recall', 'f1_score',  'auc score', 'tn', 'fp', 'fn', 'tp', 'test_score', \n",
    "  #      'train_score', 'C', 'solver', 'kernel', 'penalty', 'bootstrap', 'coef0',  \n",
    "   #    'n_estimators']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['train_function']=='train_logreg'].nlargest(3, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['train_function']=='train_SVM'].nlargest(3, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['train_function']=='train_random'].nlargest(3,'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nlargest(3, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
